{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41cb517e-0107-4228-89e8-264358755368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "sys.path.append('../learnMSA')\n",
    "import numpy as np\n",
    "from learnMSA import msa_hmm\n",
    "from matplotlib import pyplot as plt\n",
    "from src import Util"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c715bf4-a1bb-40b3-95bf-67a61b1916fa",
   "metadata": {},
   "source": [
    "## Align Refinement\n",
    "\n",
    "learnMSA leaves insertions unaligned. This is a problem, if long, important motifs (additional domains) are underrepresented in the sequences. \n",
    "\n",
    "Solution: Once a pHMM is trained, introduce a simple descision rule whether insertions should be recursively aligned. For each position $i$ where at least $k$ insertions longer than $t$ residues exists, realign those insertions (i.e. a horizontal and vertical slice of the input sequences) recursively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22294e0a-404e-42ce-ad40-22ec90059dd1",
   "metadata": {},
   "source": [
    "## Find long insertions and align them separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd2167ec-4465-44d1-83c3-99bdc0325e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_long_insertions_and_write_slice(fasta_file, lens, starts, name, t = 20, k=2, verbose=True):\n",
    "        \"\"\"\n",
    "        Finds insertions that have at least length t. If there are at least k of these sequences, writes them to file.\n",
    "        Args: \n",
    "            lens, starts: Arrays of length n where n is the number of sequences. Indicate how long insertions are and where they start respectively.\n",
    "            name: Identifier for the location of the slice.\n",
    "        \"\"\"\n",
    "        at_least_t = lens >= t\n",
    "        lengths = lens[at_least_t]\n",
    "        if lengths.size > k:\n",
    "            if verbose:\n",
    "                print(f\"Long insertions found at {name}: {lengths.size}.\")\n",
    "            which = np.squeeze(np.argwhere(at_least_t))\n",
    "            start = starts[at_least_t]\n",
    "            filename = f\"tmp/slice_{name}\"\n",
    "            to_delete = []\n",
    "            with open(filename, \"w\") as slice_file:\n",
    "                for j in range(lengths.size):\n",
    "                    aa_seq = fasta_file.aminoacid_seq_str(which[j])\n",
    "                    segment = aa_seq[start[j] : start[j] + lengths[j]]\n",
    "                    only_non_standard_aa = True\n",
    "                    for aa in msa_hmm.fasta.alphabet[:20]:\n",
    "                        if aa in segment:\n",
    "                            only_non_standard_aa = False\n",
    "                            break\n",
    "                    if only_non_standard_aa:\n",
    "                        to_delete.append(j)\n",
    "                    else:\n",
    "                        slice_file.write(\">\"+fasta_file.seq_ids[which[j]]+\"\\n\")\n",
    "                        slice_file.write(segment+\"\\n\")\n",
    "            which = np.delete(which, to_delete)\n",
    "            return (which, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a308807-a992-44c9-8999-374feda74343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAMSA (Fast and Accurate Multiple Sequence Alignment) \n",
      "  version 2.2.2- (2022-10-09)\n",
      "  S. Deorowicz, A. Debudaj-Grabysz, A. Gudys\n",
      "\n",
      "Usage:\n",
      "  famsa [options] <input_file> [<input_file_2>] <output_file>\n",
      "\n",
      "Positional parameters:\n",
      "  input_file, input_file_2 - input files in FASTA format; action depends on the number of input files:\n",
      "      * one input - multiple sequence alignment (input gaps, if present, are removed prior the alignment),\n",
      "      * two inputs - profile-profile aligment (input gaps are preserved).\n",
      "      First input can be replaced with STDIN string to read from standard input.\n",
      "  output_file - output file (pass STDOUT when writing to standard output); available outputs:\n",
      "      * alignment in FASTA format,\n",
      "      * guide tree in Newick format (-gt_export option specified),\n",
      "      * distance matrix in CSV format (-dist_export option specified),\n",
      "\n",
      "Options:\n",
      "  -help - print this message\n",
      "  -t <value> - no. of threads, 0 means all available (default: 0)\n",
      "  -v - verbose mode, show timing information (default: disabled)\n",
      "\n",
      "  -gt <sl | upgma | nj | import <file>> - guide tree method (default: sl):\n",
      "      * sl - single linkage \n",
      "      * upgma - UPGMA\n",
      "      * nj - neighbour joining\n",
      "      * import <file> - imported from a Newick file\n",
      "  -medoidtree - use MedoidTree heuristic for speeding up tree construction (default: disabled)\n",
      "  -medoid_threshold <n_seqs> - if specified, medoid trees are used only for sets with <n_seqs> or more\n",
      "  -gt_export - export a guide tree to output file in Newick format\n",
      "  -dist_export - export a distance matrix to output file in CSV format\n",
      "  -square_matrix - generate a square distance matrix instead of a default triangle\n",
      "  -pid - generate pairwise identity (the number of matching residues divided by the shorter sequence length) instead of distance\n",
      "  -keep-duplicates - keep duplicated sequences during alignment\n",
      "                     (default: disabled - duplicates are removed prior and restored after the alignment).\n",
      "\n",
      "  -gz - enable gzipped output (default: disabled)\n",
      "  -gz-lev <value> - gzip compression level [0-9] (default: 7)\n",
      "  -refine_mode <on | off | auto> - refinement mode (default: auto - the refinement is enabled for sets <= 1000 seq.)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!source activate snakeMSA ; famsa -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10a32e41-76fb-4a7b-b3cb-8803e679ebf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_aligned_insertions(alignment_model):\n",
    "    data = alignment_model.metadata[alignment_model.best_model]\n",
    "    num_seq = data.left_flank_len.shape[0]\n",
    "\n",
    "    insertions_long = []\n",
    "    for r in range(data.num_repeats):\n",
    "        insertions_long.append([])\n",
    "        for i in range(data.insertion_lens.shape[2]):\n",
    "            ins_long = find_long_insertions_and_write_slice(alignment_model.fasta_file, data.insertion_lens[r, :, i], data.insertion_start[r, :, i], f\"ins_{r}_{i}\")\n",
    "            insertions_long[-1].append(ins_long)\n",
    "    left_flank_long = find_long_insertions_and_write_slice(alignment_model.fasta_file, data.left_flank_len, data.left_flank_start, \"left_flank\")\n",
    "    right_flank_long = find_long_insertions_and_write_slice(alignment_model.fasta_file, data.right_flank_len, data.right_flank_start, \"right_flank\")\n",
    "    unannotated_long = []\n",
    "    for r in range(data.num_repeats-1):\n",
    "        unannotated_long.append(find_long_insertions_and_write_slice(alignment_model.fasta_file, data.unannotated_segments_len[r], data.unannotated_segments_start[r], f\"unannotated_{r}\"))\n",
    "        \n",
    "    slice_files = []\n",
    "    if left_flank_long is not None:\n",
    "        slice_files.append(\"tmp/slice_left_flank\")\n",
    "    if right_flank_long is not None:\n",
    "        slice_files.append(\"tmp/slice_right_flank\")\n",
    "    for r in range(data.num_repeats):\n",
    "        for i in range(data.insertion_lens.shape[2]):\n",
    "            if insertions_long[r][i] is not None:\n",
    "                slice_files.append(f\"tmp/slice_ins_{r}_{i}\")\n",
    "    for r in range(data.num_repeats-1):\n",
    "        if unannotated_long[r] is not None:\n",
    "            slice_files.append(f\"tmp/slice_unannotated_{r}\")\n",
    "            \n",
    "    #align insertions\n",
    "    for slice_file in slice_files:\n",
    "        #with learnMSA\n",
    "        #config = msa_hmm.config.make_default(alignment_model.num_models)\n",
    "        #msa_hmm.align.run_learnMSA(slice_file, slice_file+\".aln\", config, verbose=False)\n",
    "        #with t_coffee regressive\n",
    "        #!export MAX_N_PID_4_TCOFFEE=10000000 ; ~/bin/t_coffee -reg -seq {slice_file} -nseq 100 -tree mbed -method mafftginsi_msa -outfile {slice_file}.aln -quiet\n",
    "        #with clustalo\n",
    "        #!source activate clustalo ; clustalo --threads=32 -i {slice_file} -o {slice_file}.aln --force\n",
    "        #with famsa\n",
    "        !source activate snakeMSA ; famsa {slice_file} {slice_file}.aln\n",
    "        \n",
    "    #merge msa\n",
    "    insertions_long = [[(x[0], msa_hmm.fasta.Fasta(x[1]+\".aln\", aligned=True)) if x is not None else None for x in repeats] for repeats in insertions_long]\n",
    "    left_flank_long = (left_flank_long[0],  msa_hmm.fasta.Fasta(left_flank_long[1]+\".aln\", aligned=True)) if left_flank_long is not None else None\n",
    "    right_flank_long = (right_flank_long[0],  msa_hmm.fasta.Fasta(right_flank_long[1]+\".aln\", aligned=True)) if right_flank_long is not None else None\n",
    "    unannotated_long = [(x[0], msa_hmm.fasta.Fasta(x[1]+\".aln\", aligned=True)) if x is not None else None for x in unannotated_long]\n",
    "    \n",
    "    aligned_insertions = msa_hmm.alignment_model.AlignedInsertions(insertions_long, left_flank_long, right_flank_long, unannotated_long)\n",
    "    return aligned_insertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "478f7f29-da83-4a53-a18e-29b00e8e7e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runner_func(train_file, ref_file, out_file, config):\n",
    "    alignment_model = msa_hmm.align.run_learnMSA(train_file, out_file+\".default\", config, verbose=True)\n",
    "    aligned_insertions = make_aligned_insertions(alignment_model)\n",
    "    alignment_model.to_file(out_file, alignment_model.best_model, aligned_insertions = aligned_insertions)\n",
    "    return alignment_model, 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d038a63-ead3-4a21-b125-aae75c8a880e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training of 10 models on file Ald_Xan_dh_2.fasta\n",
      "Configuration: \n",
      "{\n",
      "num_models : 10\n",
      "transitioner : ProfileHMMTransitioner(\n",
      " transition_init=\n",
      "    {\n",
      "    begin_to_match : DefaultEntry() , match_to_end : DefaultExit() , \n",
      "    match_to_match : DefaultMatchTransition(1) , match_to_insert : DefaultMatchTransition(-1) , \n",
      "    insert_to_match : Norm(0, 0.1) , insert_to_insert : Norm(-0.5, 0.1) , \n",
      "    match_to_delete : DefaultMatchTransition(-1) , delete_to_match : Norm(0, 0.1) , \n",
      "    delete_to_delete : Norm(-0.5, 0.1) , left_flank_loop : Norm(0, 0.1) , \n",
      "    left_flank_exit : Norm(-1, 0.1) , right_flank_loop : Norm(0, 0.1) , \n",
      "    right_flank_exit : Norm(-1, 0.1) , unannotated_segment_loop : Norm(0, 0.1) , \n",
      "    unannotated_segment_exit : Norm(-1, 0.1) , end_to_unannotated_segment : Norm(-9, 0.1) , \n",
      "    end_to_right_flank : Norm(0, 0.1) , end_to_terminal : Norm(0, 0.1)\n",
      "    },\n",
      " flank_init=Const(0.0),\n",
      " prior=ProfileHMMTransitionPrior(match_comp=1, insert_comp=1, delete_comp=1, alpha_flank=7000, alpha_single=1000000000.0, alpha_frag=10000.0),\n",
      " frozen_kernels={})\n",
      "emitter : ProfileHMMEmitter(\n",
      " emission_init=DefaultEmission(),\n",
      " insertion_init=Const(shape=(25,)),\n",
      " prior=AminoAcidPrior(comp_count=1),\n",
      " frozen_insertions=True, )\n",
      "max_surgery_runs : 4\n",
      "length_init_quantile : 0.5\n",
      "surgery_quantile : 0.5\n",
      "min_surgery_seqs : 10000.0\n",
      "len_mul : 0.8\n",
      "batch_size : <function get_adaptive_batch_size at 0x7ff2f84211f0>\n",
      "learning_rate : 0.1\n",
      "epochs : [10, 2, 10]\n",
      "use_prior : True\n",
      "dirichlet_mix_comp_count : 1\n",
      "use_anc_probs : True\n",
      "trainable_rate_matrices : False\n",
      "surgery_del : 0.5\n",
      "surgery_ins : 0.5\n",
      "num_rate_matrices : 1\n",
      "per_matrix_rate : False\n",
      "matrix_rate_l2 : 0.0\n",
      "shared_rate_matrix : False\n",
      "equilibrium_sample : False\n",
      "transposed : False\n",
      "encoder_initializer : [Const(-3), Const(shape=(10, 20, 20)), Const(shape=(10, 20))]\n",
      "model_criterion : AIC\n",
      "encoder_weight_extractor : None\n",
      "experimental_evolve_upper_half : False\n",
      "allow_user_keys_in_config : False\n",
      "}\n",
      "Fitting models of lengths [91 82 86 87 87 86 91 93 88 90] on 2589 sequences.\n",
      "Batch size= 64 Learning rate= 0.1\n",
      "Using 1 GPUs.\n",
      "Epoch 1/10\n",
      "79/79 - 50s - loss: 242.7772 - loglik: -2.3883e+02 - logprior: -3.9430e+00\n",
      "Epoch 2/10\n",
      "79/79 - 37s - loss: 213.9706 - loglik: -2.1244e+02 - logprior: -1.5311e+00\n",
      "Epoch 3/10\n",
      "79/79 - 37s - loss: 213.1272 - loglik: -2.1160e+02 - logprior: -1.5254e+00\n",
      "Epoch 4/10\n",
      "79/79 - 36s - loss: 212.8699 - loglik: -2.1137e+02 - logprior: -1.4990e+00\n",
      "Epoch 5/10\n",
      "79/79 - 37s - loss: 213.0179 - loglik: -2.1155e+02 - logprior: -1.4690e+00\n",
      "expansions model 0: [(10, 2), (11, 1), (12, 1), (27, 1), (30, 1), (49, 2), (50, 1), (51, 4), (52, 2), (53, 2), (54, 2), (59, 1), (74, 1), (75, 1), (76, 1), (77, 1), (91, 3)]\n",
      "discards model 0: []\n",
      "expansions model 1: [(9, 3), (10, 2), (11, 2), (12, 1), (13, 1), (25, 1), (27, 1), (32, 1), (40, 2), (44, 1), (45, 4), (46, 2), (47, 3), (48, 2), (53, 1), (54, 1), (67, 1), (68, 3), (69, 3), (82, 3)]\n",
      "discards model 1: []\n",
      "expansions model 2: [(9, 3), (10, 2), (11, 2), (14, 1), (26, 1), (28, 1), (33, 1), (41, 1), (44, 1), (45, 1), (46, 1), (47, 3), (48, 2), (49, 1), (50, 2), (51, 2), (57, 1), (70, 1), (71, 3), (72, 1), (86, 3)]\n",
      "discards model 2: []\n",
      "expansions model 3: [(10, 2), (11, 1), (12, 1), (15, 1), (26, 1), (29, 1), (33, 1), (42, 1), (47, 2), (48, 4), (49, 2), (51, 2), (57, 1), (58, 1), (71, 1), (72, 3), (73, 1), (87, 3)]\n",
      "discards model 3: []\n",
      "expansions model 4: [(10, 2), (11, 1), (12, 1), (15, 1), (27, 1), (29, 1), (41, 1), (42, 1), (47, 2), (48, 4), (49, 2), (50, 2), (51, 2), (72, 1), (73, 3), (74, 3), (87, 3)]\n",
      "discards model 4: []\n",
      "expansions model 5: [(9, 3), (10, 2), (11, 1), (14, 1), (25, 1), (28, 1), (42, 1), (44, 1), (45, 2), (46, 1), (47, 4), (48, 2), (49, 2), (50, 1), (55, 1), (70, 1), (71, 3), (72, 1), (86, 3)]\n",
      "discards model 5: []\n",
      "expansions model 6: [(10, 2), (11, 1), (12, 1), (27, 1), (30, 1), (35, 1), (49, 2), (50, 3), (51, 1), (52, 1), (53, 2), (54, 1), (75, 1), (76, 3), (77, 1), (91, 3)]\n",
      "discards model 6: []\n",
      "expansions model 7: [(10, 2), (11, 1), (12, 1), (27, 1), (36, 1), (50, 2), (51, 4), (52, 2), (54, 2), (76, 1), (77, 1), (78, 1), (79, 1), (93, 3)]\n",
      "discards model 7: []\n",
      "expansions model 8: [(9, 3), (10, 2), (11, 1), (26, 1), (29, 1), (34, 1), (42, 1), (47, 1), (48, 4), (49, 2), (51, 2), (52, 1), (57, 1), (72, 1), (73, 1), (74, 1), (75, 2), (88, 3)]\n",
      "discards model 8: []\n",
      "expansions model 9: [(10, 2), (11, 1), (12, 1), (15, 1), (27, 1), (29, 1), (43, 1), (48, 2), (49, 4), (50, 2), (52, 2), (53, 1), (74, 1), (75, 3), (76, 1), (90, 3)]\n",
      "discards model 9: []\n",
      "Re-initialized the encoder parameters.\n",
      "Fitting models of lengths [118, 120, 120, 116, 118, 118, 116, 116, 117, 117] on 2589 sequences.\n",
      "Batch size= 64 Learning rate= 0.1\n",
      "Using 1 GPUs.\n",
      "Epoch 1/2\n",
      "79/79 - 52s - loss: 204.1565 - loglik: -2.0117e+02 - logprior: -2.9888e+00\n",
      "Epoch 2/2\n",
      "79/79 - 36s - loss: 197.5096 - loglik: -1.9743e+02 - logprior: -7.5799e-02\n",
      "expansions model 0: [(65, 1), (72, 1), (118, 3)]\n",
      "discards model 0: [ 11  55  56  62  63  68  71  72 115 116 117]\n",
      "expansions model 1: [(67, 1), (120, 3)]\n",
      "discards model 1: [ 13  16  62  63  65  66  72  73 102 117 118 119]\n",
      "expansions model 2: [(68, 2), (120, 3)]\n",
      "discards model 2: [ 12  16  56  58  63  66  67  71  74 117 118 119]\n",
      "expansions model 3: [(68, 1), (69, 1), (116, 3)]\n",
      "discards model 3: [ 57  61  62  68  69 113 114 115]\n",
      "expansions model 4: [(64, 1), (71, 1), (118, 3)]\n",
      "discards model 4: [ 61  62  67  70  71 100 115 116 117]\n",
      "expansions model 5: [(57, 2), (73, 2), (118, 3)]\n",
      "discards model 5: [ 12  57  62  69  72 115 116 117]\n",
      "expansions model 6: [(68, 1), (69, 1), (116, 3)]\n",
      "discards model 6: [ 11  57  60  67  68  70 113 114 115]\n",
      "expansions model 7: [(68, 1), (69, 2), (116, 3)]\n",
      "discards model 7: [ 10  57  61  62  68  69 113 114 115]\n",
      "expansions model 8: [(70, 1), (71, 1), (117, 3)]\n",
      "discards model 8: [ 13  61  62  68  69  71 114 115 116]\n",
      "expansions model 9: [(68, 2), (72, 1), (117, 3)]\n",
      "discards model 9: [ 57  60  61  68  69  71 114 115 116]\n",
      "Re-initialized the encoder parameters.\n",
      "Fitting models of lengths [112, 112, 113, 113, 114, 117, 112, 113, 113, 114] on 2589 sequences.\n",
      "Batch size= 64 Learning rate= 0.1\n",
      "Using 1 GPUs.\n",
      "Epoch 1/2\n",
      "79/79 - 53s - loss: 201.9579 - loglik: -1.9969e+02 - logprior: -2.2673e+00\n",
      "Epoch 2/2\n",
      "79/79 - 37s - loss: 197.3591 - loglik: -1.9784e+02 - logprior: 0.4817\n",
      "expansions model 0: [(54, 1), (66, 2), (112, 3)]\n",
      "discards model 0: [ 60  66 109 110 111]\n",
      "expansions model 1: [(55, 1), (62, 2), (67, 1), (112, 3)]\n",
      "discards model 1: [ 11  61 109 110 111]\n",
      "expansions model 2: [(55, 1), (63, 1), (68, 1), (113, 3)]\n",
      "discards model 2: [ 11  61  62 110 111 112]\n",
      "expansions model 3: [(65, 1), (66, 1), (113, 3)]\n",
      "discards model 3: [ 10  65  66 110 111 112]\n",
      "expansions model 4: [(62, 1), (67, 1), (68, 2), (114, 3)]\n",
      "discards model 4: [ 10  56  62  68 111 112 113]\n",
      "expansions model 5: [(71, 2), (117, 3)]\n",
      "discards model 5: [ 11  56  57  65  70  71 114 115 116]\n",
      "expansions model 6: [(64, 1), (65, 1), (66, 1), (112, 3)]\n",
      "discards model 6: [ 64  65 109 110 111]\n",
      "expansions model 7: [(64, 1), (66, 2), (113, 3)]\n",
      "discards model 7: [ 64  66 110 111 112]\n",
      "expansions model 8: [(55, 1), (66, 1), (67, 2), (113, 3)]\n",
      "discards model 8: [ 11  65  67 110 111 112]\n",
      "expansions model 9: [(66, 1), (67, 1), (68, 1), (114, 3)]\n",
      "discards model 9: [ 10  65  66  68 111 112 113]\n",
      "Re-initialized the encoder parameters.\n",
      "Fitting models of lengths [113, 114, 113, 112, 114, 113, 113, 114, 114, 113] on 2589 sequences.\n",
      "Batch size= 64 Learning rate= 0.1\n",
      "Using 1 GPUs.\n",
      "Epoch 1/10\n",
      "79/79 - 54s - loss: 201.8901 - loglik: -1.9985e+02 - logprior: -2.0448e+00\n",
      "Epoch 2/10\n",
      "79/79 - 36s - loss: 197.4443 - loglik: -1.9808e+02 - logprior: 0.6313\n",
      "Epoch 3/10\n",
      "79/79 - 37s - loss: 196.8954 - loglik: -1.9760e+02 - logprior: 0.7002\n",
      "Epoch 4/10\n",
      "79/79 - 37s - loss: 196.9912 - loglik: -1.9773e+02 - logprior: 0.7358\n",
      "Time for alignment: 571.5236\n",
      "Likelihoods:  ['-197.3067', '-197.3138', '-197.3423', '-197.5984', '-197.5803', '-197.3334', '-197.1308', '-197.1177', '-197.1801', '-197.7385']\n",
      "Mean likelihood:  -197.36419966761417\n",
      "Selection criterion: AIC\n",
      "Best model:  7 (0-based)\n",
      "time for generating output: 2.0639\n",
      "Wrote file test.Ald_Xan_dh_2.fasta.default\n",
      "Long insertions found at ins_0_65: 4.\n",
      "Long insertions found at left_flank: 4.\n",
      "Long insertions found at right_flank: 63.\n",
      "FAMSA (Fast and Accurate Multiple Sequence Alignment) \n",
      "  version 2.2.2- (2022-10-09)\n",
      "  S. Deorowicz, A. Debudaj-Grabysz, A. Gudys\n",
      "\n",
      "Done!\n",
      "FAMSA (Fast and Accurate Multiple Sequence Alignment) \n",
      "  version 2.2.2- (2022-10-09)\n",
      "  S. Deorowicz, A. Debudaj-Grabysz, A. Gudys\n",
      "\n",
      "Done!\n",
      "FAMSA (Fast and Accurate Multiple Sequence Alignment) \n",
      "  version 2.2.2- (2022-10-09)\n",
      "  S. Deorowicz, A. Debudaj-Grabysz, A. Gudys\n",
      "\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<learnMSA.msa_hmm.AlignmentModel.AlignmentModel at 0x7ff5171c5970>, 0.0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runner_func(\"data/homfam/train/Ald_Xan_dh_2.fasta\", \"\", \"test.Ald_Xan_dh_2.fasta\", msa_hmm.config.make_default(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "94d51fd0-104a-4edf-82ad-078f4b9be1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERE: 1dgja\n",
      "HERE: 1hlra\n",
      "HERE: 1fo4a\n",
      "HERE: 1jrob\n",
      "HERE: 1ffvb\n",
      "HERE: 1n62b\n"
     ]
    }
   ],
   "source": [
    "!id_list=$(sed -n '/^>/p' \"data/homfam/refs/Ald_Xan_dh_2.ref\" | sed 's/^.//'); export MAX_N_PID_4_TCOFFEE=10000000 ; ~/bin/t_coffee -other_pg seq_reformat -in test.Ald_Xan_dh_2.fasta -action +extract_seq_list \"${id_list[@]}\" +rm_gap > test/data/projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e313723d-ea1d-49ca-9b28-b70654456a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************************\n",
      "seq1       seq2          Sim   [ALL]           Tot  \n",
      "Ald_Xan_dh_2  6          31.8    82.8 [100.0]   [21304]\n"
     ]
    }
   ],
   "source": [
    "!export MAX_N_PID_4_TCOFFEE=10000000 ; ~/bin/t_coffee -other_pg aln_compare -al1 data/homfam/refs/Ald_Xan_dh_2.ref -al2 test/data/projection -compare_mode sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d08e65e-21b0-4f67-8a89-addc240a3d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: results/align_insertions_famsa/models/cys/model/assets\n",
      "INFO:tensorflow:Assets written to: results/align_insertions_famsa/models/DMRL_synthase/model/assets\n",
      "INFO:tensorflow:Assets written to: results/align_insertions_famsa/models/Stap_Strp_toxin/model/assets\n",
      "WARNING:tensorflow:5 out of the last 15 calls to <function get_state_expectations.<locals>.batch_posterior_state_probs at 0x7ff35f75dca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "INFO:tensorflow:Assets written to: results/align_insertions_famsa/models/ghf5/model/assets\n",
      "INFO:tensorflow:Assets written to: results/align_insertions_famsa/models/myb_DNA-binding/model/assets\n",
      "INFO:tensorflow:Assets written to: results/align_insertions_famsa/models/ghf10/model/assets\n",
      "INFO:tensorflow:Assets written to: results/align_insertions_famsa/models/il8/model/assets\n",
      "WARNING:tensorflow:5 out of the last 22 calls to <function get_state_expectations.<locals>.batch_posterior_state_probs at 0x7ff39ca64700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "INFO:tensorflow:Assets written to: results/align_insertions_famsa/models/cytb/model/assets\n",
      "INFO:tensorflow:Assets written to: results/align_insertions_famsa/models/kringle/model/assets\n",
      "INFO:tensorflow:Assets written to: results/align_insertions_famsa/models/LIM/model/assets\n",
      "INFO:tensorflow:Assets written to: results/align_insertions_famsa/models/annexin/model/assets\n",
      "INFO:tensorflow:Assets written to: results/align_insertions_famsa/models/hormone_rec/model/assets\n",
      "INFO:tensorflow:Assets written to: results/align_insertions_famsa/models/Acetyltransf/model/assets\n",
      "INFO:tensorflow:Assets written to: results/align_insertions_famsa/models/phoslip/model/assets\n",
      "INFO:tensorflow:Assets written to: results/align_insertions_famsa/models/cah/model/assets\n",
      "INFO:tensorflow:Assets written to: results/align_insertions_famsa/models/trfl/model/assets\n",
      "INFO:tensorflow:Assets written to: results/align_insertions_famsa/models/serpin/model/assets\n"
     ]
    }
   ],
   "source": [
    "name = \"align_insertions_famsa\"\n",
    "Util.run_tests(runner_func, name, datasets=[\"homfam\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f3eaca2-4ff5-4f70-b77c-202083920ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export MAX_N_PID_4_TCOFFEE=10000000 ; ~/bin/t_coffee -reg -seq tmp/slice_ins_0_51 -nseq 100 -tree mbed -method mafftginsi_msa -outfile tmp/slice_ins_0_51.aln -quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd697625-8021-4af0-aec6-700d48e181c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83.90425531914892"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Util.get_score(\"align_insertions_t_coffee\", \"homfam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "352b69a3-eee2-49ef-9ce3-e4acd6bb080a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83.90744680851066"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Util.get_score(\"align_insertions_famsa\", \"homfam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f68f090f-818e-49e0-b1f5-a33900dd11b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83.7489361702128"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Util.get_score(\"align_insertions_clustalo\", \"homfam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16e918b0-c96f-4089-bdc1-d135b9de29b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83.60106382978722"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Util.get_score(\"reg\", \"homfam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422b2071-234a-45df-805f-68c7fccd89fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
