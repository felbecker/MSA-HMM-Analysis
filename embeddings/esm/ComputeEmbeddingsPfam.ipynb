{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87283744-7157-4aa9-850e-d1bb0f7cbcd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fair-esm in /home/jovyan/.local/lib/python3.10/site-packages (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install fair-esm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6148d3-ae7e-4443-885e-d5b4b02a886c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0667e323-d431-41bc-8a91-cb31969e6405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'esm' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/facebookresearch/esm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ac2afb-d4c3-453f-b673-d9b6e252ff49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('../../../learnMSA')\n",
    "from learnMSA import msa_hmm\n",
    "import numpy as np\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eafc615-bbd0-4d7c-8a04-1f6e44d6335b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48bed75-c0e0-4e64-9280-9d2405803e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p pfam\n",
    "data_path = \"../../../PFAM/no_gaps\"\n",
    "for file in os.listdir(data_path)[-300:]:\n",
    "    \n",
    "    if file.endswith(\".fasta\"):\n",
    "        family = \".\".join(file.split(\".\")[:-1])\n",
    "        \n",
    "        if os.path.exists(f\"pfam/{family}.npy\"):\n",
    "            continue\n",
    "            \n",
    "        if not os.path.exists(f\"pfam/{family}\"):\n",
    "            #generates embeddings if they do not exist so far\n",
    "            !python esm/scripts/extract.py esm2_t36_3B_UR50D ../../../PFAM/no_gaps/{family}.fasta pfam/{family} --include per_tok >> log.txt\n",
    "            \n",
    "        #concatenates all embeddings to numpy arrays and stores them family-wise in a single file\n",
    "        fasta_file = msa_hmm.fasta.Fasta(data_path+\"/\"+file, single_seq_ok=True)\n",
    "        try:\n",
    "            embeddings = []\n",
    "            for seqid in fasta_file.seq_ids:\n",
    "                emb = torch.load(f\"pfam/{family}/{seqid}.pt\")\n",
    "                emb_np = emb[\"representations\"][36].detach().numpy()\n",
    "                embeddings.append(emb_np)\n",
    "\n",
    "            #concatenate all sequences together in the order of the fasta file\n",
    "            #we can get them back with that same order and their lengths\n",
    "            embeddings = np.concatenate(embeddings, 0)    \n",
    "            np.save(f\"pfam/{family}.npy\", embeddings)\n",
    "        \n",
    "            #remove the torch embeddings (too many single files)\n",
    "            !rm -r pfam/{family}\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Embeddings for family {family} are missing. Skipping this family\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b890675-8ede-4398-943f-9ce688748e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "data_path = \"../../../PFAM/no_gaps\"\n",
    "files = [\"PF10249.fasta\", \"PF06388.fasta\"]\n",
    "for file in files:\n",
    "    family = \".\".join(file.split(\".\")[:-1])\n",
    "    embeddings = np.load(f\"pfam/{family}.npy\")\n",
    "    fasta_file = msa_hmm.fasta.Fasta(data_path+\"/\"+file)\n",
    "    len_sum = 0\n",
    "    for seqid, length in zip(fasta_file.seq_ids, fasta_file.seq_lens):\n",
    "        emb = torch.load(f\"pfam/{family}/{seqid}.pt\")\n",
    "        emb_np = emb[\"representations\"][36].detach().numpy()\n",
    "        assert np.all(emb_np == embeddings[len_sum : len_sum + length])\n",
    "        len_sum += length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43f06fca-9f28-41a5-9462-1660940c2842",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r ~/.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ad75ca-b0a6-4285-bc62-f9a948a86e38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
