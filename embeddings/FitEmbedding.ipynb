{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98b903e9-5ed1-4da6-a51c-1faa47f21841",
   "metadata": {},
   "source": [
    "# A symmetric bilinear form for embeddings\n",
    "\n",
    "In this notebook, we use a simple bilinear symmetric model to classify pairs of embeddings as homolog (i.e. whether the corresponding residues are in the same column in a reference MSA).\n",
    "\n",
    "Let $x,y$ be $d$-dimensional embeddings. The model is\n",
    "\n",
    "$$f(x,y) = \\sigma(x^T R R^T y + b)$$\n",
    "\n",
    "where $R \\in \\mathbb{R}^{d \\times k}$ is a parameter matrix and $k<d$ can be controlled for dimensionality reduction. $b$ is a scalar bias.\n",
    "\n",
    "The model is trained in a binary classification task, where the homology labels are induced by reference multiple sequence alignments, i.e. the label is 1 if the positions of two embeddings share the same column and 0 otherwise.\n",
    "\n",
    "When used in learnMSA, we can compute embedding-based emission probabilities for an $d$-dimensional embeddings $x$ and a $k$-dimensional match kernel $m$ with pre-trained $R$ and $b$ like so:\n",
    "\n",
    "$P(x \\mid m) = \\sigma(x R m + b)$.\n",
    "\n",
    "In this case only $m$ is learned and $R$ and $b$ are fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "092e1167-3aca-4239-ae1b-21fe2856988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../../learnMSA')\n",
    "from learnMSA import msa_hmm\n",
    "import os\n",
    "from BilinearSymmetric import SymmetricBilinearReduction, BackgroundEmbedding\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import io\n",
    "import os\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71309a30-3a2f-492a-bd15-cc6b47b8cedd",
   "metadata": {},
   "source": [
    "## Data preparation Task 1: Binary classification of embeddings sharing a column\n",
    "\n",
    "Use Pfams clan hierarchy as the basis for train/test splitting and batch sampling. A clan is a collection of related Pfam entries. The relationship may be defined by similarity of sequence, structure or profile-HMM.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8db1c60e-7a78-4e94-a829-f507a9cc7307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PF00001\tCL0192\tGPCR_A\t7tm_1\t7 transmembrane receptor (rhodopsin family)\n",
      "PF00002\tCL0192\tGPCR_A\t7tm_2\t7 transmembrane receptor (Secretin family)\n",
      "PF00003\tCL0192\tGPCR_A\t7tm_3\t7 transmembrane sweet-taste receptor of 3 GCPR\n",
      "PF00004\tCL0023\tP-loop_NTPase\tAAA\tATPase family associated with various cellular activities (AAA)\n",
      "PF00005\tCL0023\tP-loop_NTPase\tABC_tran\tABC transporter\n",
      "PF00006\tCL0023\tP-loop_NTPase\tATP-synt_ab\tATP synthase alpha/beta family, nucleotide-binding domain\n",
      "PF00007\tCL0079\tCystine-knot\tCys_knot\tCystine-knot domain\n",
      "PF00008\tCL0001\tEGF\tEGF\tEGF-like domain\n",
      "PF00009\tCL0023\tP-loop_NTPase\tGTP_EFTU\tElongation factor Tu GTP binding domain\n",
      "PF00010\t\t\tHLH\tHelix-loop-helix DNA-binding domain\n"
     ]
    }
   ],
   "source": [
    "!head ../../PFAM/Pfam-A.clans.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db49c911-6ea4-4984-9036-977589f03cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clans_df = pd.read_csv(\"../../PFAM/Pfam-A.clans.tsv\", header=None, sep=\"\\t\")\n",
    "clans_df[1] = clans_df[1].fillna(clans_df[0]) #families with no clan become their own clans\n",
    "clans_df.set_index(0, inplace=True)\n",
    "clans_df.drop([2,3,4], axis=1, inplace=True)\n",
    "clans_df.rename(columns={1 : \"clan\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40af9da0-0c31-47f5-8421-200bc10f9997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CL0192'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clans_df.loc[\"PF00001\"].clan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7edc3b57-e339-42e3-9825-4da22a5b158b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11865 clans in total\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clan</th>\n",
       "      <th>fasta_index</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PF00001</th>\n",
       "      <td>CL0192</td>\n",
       "      <td>13877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PF00002</th>\n",
       "      <td>CL0192</td>\n",
       "      <td>12061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PF00003</th>\n",
       "      <td>CL0192</td>\n",
       "      <td>12652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PF00004</th>\n",
       "      <td>CL0023</td>\n",
       "      <td>6410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PF00005</th>\n",
       "      <td>CL0023</td>\n",
       "      <td>13836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           clan  fasta_index\n",
       "0                           \n",
       "PF00001  CL0192        13877\n",
       "PF00002  CL0192        12061\n",
       "PF00003  CL0192        12652\n",
       "PF00004  CL0023         6410\n",
       "PF00005  CL0023        13836"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(77)\n",
    "\n",
    "#sequences longer than this value were truncated by the LM \n",
    "#todo: redo the embeddings without truncation\n",
    "#for simplicity we omit families with at least one sequence longer than this value (0.3% of all families)\n",
    "lm_model_truncation_value = 1022\n",
    "\n",
    "def get_family(filepath):\n",
    "    return \".\".join(os.path.basename(filepath).split(\".\")[:-1])\n",
    "\n",
    "#load all fasta files\n",
    "#(takes a while)\n",
    "datasets = \"../../PFAM/alignments/\"\n",
    "ext = \".fasta\"\n",
    "\n",
    "clans_df[\"fasta_index\"] = np.nan\n",
    "\n",
    "# load all ref alignments\n",
    "fasta_files = []\n",
    "to_drop = []\n",
    "truncated_clans = set()\n",
    "for file in os.listdir(datasets):\n",
    "    if file.endswith(ext):\n",
    "        family = \".\".join(file.split(\".\")[:-1])\n",
    "        fasta = msa_hmm.fasta.Fasta(datasets+file, aligned=True, single_seq_ok=True)\n",
    "        #omit families with only one sequence or families with at least one truncated sequence\n",
    "        if fasta.num_seq == 1 or np.any(fasta.seq_lens > lm_model_truncation_value):\n",
    "            to_drop.append(family)\n",
    "            truncated_clans.add(clans_df.loc[family].clan)\n",
    "            continue\n",
    "        fasta_files.append(fasta)\n",
    "        clans_df.loc[family, \"fasta_index\"] = len(fasta_files)-1\n",
    "        \n",
    "#drop families\n",
    "clans_df = clans_df.drop(to_drop, axis=0)\n",
    "\n",
    "assert not clans_df.isna().values.any()\n",
    "clans_df = clans_df.astype({\"fasta_index\": \"int32\"})\n",
    "\n",
    "#preprocessing\n",
    "seq_lens, starting_pos, seq_pos_to_column = {}, {}, {}\n",
    "for f in fasta_files:\n",
    "    family = get_family(f.filename)\n",
    "    seq_lens[family] = f.seq_lens\n",
    "    starting_pos[family] = f.starting_pos\n",
    "    seq_pos_to_column[family] = f.membership_targets\n",
    "\n",
    "unique_clans = clans_df.clan.unique()\n",
    "num_clans = unique_clans.size\n",
    "print(f\"{num_clans} clans in total\")\n",
    "clans_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d33b3695-bad0-4e0b-9caf-177d97052904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8T\tesm/pfam\n"
     ]
    }
   ],
   "source": [
    "#cannot load all precomputed embeddings at once into memory\n",
    "#and loading them on demand in the data pipeline is too slow\n",
    "#with a memory limit of 100GB \n",
    "#assuming that a single embedding vector has size 2560 and the datatype is float16\n",
    "#one embedding is 2560 * 2 /1000 /1000 = 0.00512MB\n",
    "#therefore we can store 100000 / 0.00512 = 19531250 embeddings\n",
    "#thats about 16 per sequence \n",
    "#and about 1560 per clan\n",
    "!du -h esm/pfam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "760c853c-302c-4eba-8b29-26bd4f25605e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prot_model = \"esm\"\n",
    "!mkdir -p {prot_model}/tmp\n",
    "embeddings_per_clan = 1500\n",
    "    \n",
    "if not os.path.exists(\"esm/sampled_embeddings.npy\"):\n",
    "    def load(clan):\n",
    "        if os.path.exists(f\"{prot_model}/tmp/{clan}_embeddings.npy\"):\n",
    "            return\n",
    "        clan_families = clans_df[clans_df.clan == clan]\n",
    "        clan_embeddings = {family : np.load(f\"{prot_model}/pfam/{family}.npy\").astype(np.float16) for family in clan_families.index}\n",
    "        family_sample = clan_families.sample(embeddings_per_clan, replace=True).index\n",
    "        emb, fam, col = [], [], []\n",
    "        for j, family in enumerate(family_sample):\n",
    "            seq = np.random.randint(seq_lens[family].size, size=1)[0]\n",
    "            pos_in_seq = np.random.randint(seq_lens[family][seq], size=1)[0]\n",
    "            emb.append( clan_embeddings[family][starting_pos[family][seq] + pos_in_seq] )\n",
    "            fam.append( clans_df.index.get_loc(family) )\n",
    "            col.append( seq_pos_to_column[family][starting_pos[family][seq] + pos_in_seq] )\n",
    "        np.save(f\"{prot_model}/tmp/{clan}_embeddings.npy\", np.stack(emb, axis=0))\n",
    "        np.save(f\"{prot_model}/tmp/{clan}_families.npy\", np.array(fam))\n",
    "        np.save(f\"{prot_model}/tmp/{clan}_columns.npy\", np.array(col))\n",
    "\n",
    "    with Pool(8) as p:\n",
    "        p.map(load, unique_clans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f287b36-a280-494d-84d2-269ccf13bf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"esm/sampled_embeddings.npy\"):\n",
    "    total_num_embeddings = num_clans * embeddings_per_clan\n",
    "    emb_dim = 2560\n",
    "    embeddings = np.zeros((num_clans, embeddings_per_clan, emb_dim), dtype=np.float16)\n",
    "    families = np.zeros((num_clans, embeddings_per_clan), dtype=np.int32)\n",
    "    columns = np.zeros((num_clans, embeddings_per_clan), dtype=np.int32)\n",
    "    for i, clan in enumerate(unique_clans):\n",
    "        emb = np.load(f\"{prot_model}/tmp/{clan}_embeddings.npy\")\n",
    "        fam = np.load(f\"{prot_model}/tmp/{clan}_families.npy\")\n",
    "        col = np.load(f\"{prot_model}/tmp/{clan}_columns.npy\")\n",
    "        embeddings[i] = emb\n",
    "        families[i] = fam\n",
    "        columns[i] = col\n",
    "    np.save(f\"{prot_model}/sampled_embeddings.npy\", embeddings)\n",
    "    np.save(f\"{prot_model}/sampled_families.npy\", families)\n",
    "    np.save(f\"{prot_model}/sampled_columns.npy\", columns)\n",
    "else:\n",
    "    sampled_embeddings = np.load(f\"{prot_model}/sampled_embeddings.npy\")\n",
    "    sampled_families = np.load(f\"{prot_model}/sampled_families.npy\")\n",
    "    sampled_columns = np.load(f\"{prot_model}/sampled_columns.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b74ad917-267b-4a45-a27e-33f7cc118ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = sampled_embeddings.shape[-1]\n",
    "A = np.arange(embeddings_per_clan)\n",
    "def make_dataset(clans, batch_size):\n",
    "    def _gen_inputs():\n",
    "        \"\"\" Generates one batch of training examples by drawing a random embedding pair.\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            #sample random clans\n",
    "            c = np.random.choice(clans, size=batch_size)\n",
    "            \n",
    "            #sample a random embeddings from clans\n",
    "            i = np.random.randint(embeddings_per_clan, size=batch_size)\n",
    "            \n",
    "            #sample another random embedding from the same family as the first one\n",
    "            batch_families = sampled_families[c, i]\n",
    "            rand = np.random.rand(batch_size)\n",
    "            j = []\n",
    "            for clan, f, r in zip(sampled_families[c], batch_families, rand):\n",
    "                same_family = A[clan == f]\n",
    "                j.append(same_family[np.floor(r * same_family.size).astype(i.dtype)])\n",
    "            j = np.array(j)\n",
    "            \n",
    "            #label is 1 if and only if they share the same alignment column\n",
    "            label = np.float32(sampled_columns[c,i] == sampled_columns[c,j])\n",
    "            yield (sampled_embeddings[c,i], sampled_embeddings[c,j]), np.reshape(label, (batch_size))\n",
    "            \n",
    "    output_signature = ((tf.TensorSpec(shape=(batch_size, emb_dim), dtype=tf.float32), \n",
    "                         tf.TensorSpec(shape=(batch_size, emb_dim), dtype=tf.float32)), \n",
    "                            tf.TensorSpec(shape=(batch_size), dtype=tf.float32))\n",
    "    ds = tf.data.Dataset.from_generator(_gen_inputs, output_signature = output_signature)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae050f52-2c6e-41f2-b2fa-8d2c02cbc7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split by clan, test on kept back clans\n",
    "test_p = 0.15\n",
    "val_p = 0.15\n",
    "assert test_p + val_p < 1\n",
    "num_test = int(num_clans * test_p)\n",
    "num_val = int(num_clans * val_p)\n",
    "num_train = num_clans - num_test\n",
    "a = np.arange(num_clans)\n",
    "np.random.shuffle(a)\n",
    "test_clans = a[:num_test]\n",
    "val_clans = a[num_test:num_test+num_val]\n",
    "train_clans = a[num_test+num_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ee531ad-9b2d-4fa2-8d7a-1557c882ec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "test_ds = make_dataset(test_clans, batch_size)\n",
    "val_ds = make_dataset(val_clans, batch_size)\n",
    "train_ds = make_dataset(train_clans, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05be1b58-d6f5-432d-851b-99125ed5a4a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011054687"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([y for x,y in test_ds.take(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "002f6cb2-5806-4965-810c-25dd13b97c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010488281"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([y for x,y in train_ds.take(100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb5f6bc-0626-401b-aed6-d5fb2759f124",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a39cff3d-2e1a-4673-b4f3-025b29cfd667",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tackle the class imbalance problem\n",
    "def make_weighted_bce(pos_weight):\n",
    "    bce = tf.keras.losses.BinaryCrossentropy()\n",
    "    def weighted_bce(y_true, y_pred):\n",
    "        weights = y_true * pos_weight + (1-y_true)\n",
    "        return bce(y_true, y_pred, sample_weight=weights)\n",
    "    return weighted_bce\n",
    "    \n",
    "def make_model(reduced_dim = 256, dropout = 0.2, l2 = 0., pos_weight=1.):\n",
    "    # input to the training pipeline are pairs of embeddings\n",
    "    emb1 = tf.keras.layers.Input(shape=(emb_dim,))\n",
    "    emb2 = tf.keras.layers.Input(shape=(emb_dim,))\n",
    "\n",
    "    # outputs are homology probabilities \n",
    "    output = SymmetricBilinearReduction(reduced_dim, dropout, l2)(emb1[:,tf.newaxis], emb2[:,tf.newaxis])\n",
    "\n",
    "    # construct a model and compile for a standard binary classification task\n",
    "    model = tf.keras.models.Model(inputs=[emb1, emb2], outputs=output)\n",
    "\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=1e-2,\n",
    "        decay_steps=1000,\n",
    "        decay_rate=0.9)\n",
    "\n",
    "    model.compile(loss=make_weighted_bce(pos_weight), \n",
    "                  optimizer=tf.keras.optimizers.Adam(lr_schedule),\n",
    "                  metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9443b3-0e8b-4c72-b849-2dcf17d9619c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{prot_model}/fit_log.txt\", \"w\") as file:\n",
    "    for dim in [64, 128, 256]:\n",
    "        for dropout in [0.1, 0.2]:\n",
    "            model = make_model(dim, dropout)\n",
    "            model.fit(train_ds, epochs=20, steps_per_epoch=10000, verbose=0)\n",
    "            file.write(f\"dim = {dim} dropout = {dropout}\\n\")\n",
    "            file.write(f\"val results = {model.evaluate(val_ds, steps=10000, verbose=0)}\\n\")\n",
    "            file.write(f\"test results = {model.evaluate(test_ds, steps=10000, verbose=0)}\\n\")\n",
    "            file.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17be886b-82bb-43b6-b735-66b638233613",
   "metadata": {},
   "source": [
    "## Retrain the best configuration on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1a69fb0e-05cf-47d1-937f-82b531353696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 104s 10ms/step - loss: 0.3559 - accuracy: 0.9374 - precision_10: 0.1221 - recall_10: 0.7476\n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 104s 10ms/step - loss: 0.1929 - accuracy: 0.9849 - precision_10: 0.4035 - recall_10: 0.7302\n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 105s 10ms/step - loss: 0.1303 - accuracy: 0.9897 - precision_10: 0.5289 - recall_10: 0.7723\n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 105s 10ms/step - loss: 0.1023 - accuracy: 0.9910 - precision_10: 0.5713 - recall_10: 0.7909\n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 104s 10ms/step - loss: 0.0911 - accuracy: 0.9915 - precision_10: 0.5906 - recall_10: 0.8011\n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 105s 10ms/step - loss: 0.0871 - accuracy: 0.9916 - precision_10: 0.5970 - recall_10: 0.8055\n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 105s 10ms/step - loss: 0.0852 - accuracy: 0.9918 - precision_10: 0.6005 - recall_10: 0.8081\n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 104s 10ms/step - loss: 0.0849 - accuracy: 0.9918 - precision_10: 0.6010 - recall_10: 0.8069\n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 105s 10ms/step - loss: 0.0845 - accuracy: 0.9917 - precision_10: 0.5952 - recall_10: 0.8043\n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 104s 10ms/step - loss: 0.0844 - accuracy: 0.9918 - precision_10: 0.6013 - recall_10: 0.8078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dropout_layer_call_fn, dropout_layer_call_and_return_conditional_losses, dropout_layer_call_fn, dropout_layer_call_and_return_conditional_losses, dropout_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: esm/bilinear_form_model_pfam_128_0.1_0_4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: esm/bilinear_form_model_pfam_128_0.1_0_4/assets\n"
     ]
    }
   ],
   "source": [
    "reduced_dim = 128\n",
    "dropout = 0.1\n",
    "l2 = 0\n",
    "pos_weight = 4\n",
    "full_ds = make_dataset(a, batch_size)\n",
    "final_model = make_model(reduced_dim = reduced_dim, dropout = dropout, l2 = l2, pos_weight = pos_weight)\n",
    "final_model.fit(full_ds, epochs=10, steps_per_epoch=10000)\n",
    "final_model.save(f\"{prot_model}/bilinear_form_model_pfam_{reduced_dim}_{dropout}_{l2}_{pos_weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe56a959-d23e-4659-8203-4d2b11602d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model(f\"{prot_model}/bilinear_form_model_pfam_{reduced_dim}_{dropout}_{l2}_{pos_weight}\", \n",
    "                                          custom_objects = {\"SymmetricBilinearReduction\" : SymmetricBilinearReduction, \n",
    "                                                            \"weighted_bce\" : make_weighted_bce(pos_weight)})\n",
    "loaded_model.layers[-1].b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "72a6d2b7-b6b5-4d52-a800-feb031f34963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'symmetric_bilinear_reduction_7/b:0' shape=(1,) dtype=float32, numpy=array([-39.687576], dtype=float32)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = tf.keras.models.load_model(f\"{prot_model}/bilinear_form_model_pfam_{reduced_dim}_{dropout}_{0}_{pos_weight}\", \n",
    "                                          custom_objects = {\"SymmetricBilinearReduction\" : SymmetricBilinearReduction, \n",
    "                                                            \"weighted_bce\" : make_weighted_bce(pos_weight)})\n",
    "loaded_model.layers[-1].b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3770acc5-a34e-4e8f-8a86-9507884e9bfc",
   "metadata": {},
   "source": [
    "## Test if the model can be loaded again without errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "57203b2d-9d02-42b1-bd8d-b65de4278259",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model(f\"{prot_model}/bilinear_form_model_pfam_{reduced_dim}_{dropout}_{l2}_{pos_weight}\", \n",
    "                                          custom_objects = {\"SymmetricBilinearReduction\" : SymmetricBilinearReduction, \n",
    "                                                            \"weighted_bce\" : make_weighted_bce(pos_weight)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0c9194ef-604e-4a3b-a146-957581b25189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.0416 - accuracy: 0.9954 - precision_9: 0.8133 - recall_9: 0.7791\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.04159269854426384,\n",
       " 0.995437502861023,\n",
       " 0.8132928609848022,\n",
       " 0.7790518403053284]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.evaluate(test_ds, steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45ecd2a-736e-40de-b5f5-6fd7025028b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
